---
title: "Semana 15 y 16: LiDar Semántico y comparación con dataset"
excerpt_separator: "<!--more-->"
categories:
  - Blog
tags:
  - CARLA
  - Unreal
  - PythonAPI
  - LiDar
  - Datasets
---

Se ha trabajado con el LiDAR semántico en CARLA para obtener etiquetas precisas en cada punto de la captura, lo que resulta fundamental para la generación de datasets sintéticos.

## Semantic LiDar
Se ha determinado que para futuros datasets sintéticos creados en CARLA Simulator erá imprescindible el uso del LiDar semántico de CARLA,
ya que este proporciona información detallada sobre la etiqueta correspondiente a cada punto registrado.

### Modificación del código: `o3d-lidarViz-manual-segmentation.py`
Se ha modificado el script original `o3d-lidarViz-manual-segmentation.py` para incorporar el LiDAR semántico, dando lugar al nuevo `o3d-semanticLidarViz-manual-segmentation.py`.
En particular, se ha cambiado la búsqueda del blueprint en la función `spawn_vehicle_lidar_camera_segmentation()` para utilizar el sensor semántico:

```python
 lidar_bp = bp.find('sensor.lidar.ray_cast_semantic')
```

También se tuvo que modificar el `lidar_callback()` debido a que la información que hay una mayor cantidad de datos que procesar en el Semantic LiDar en comparación al LiDar convencional. 
En la `raw_data`, a parte de las coordenadas (X, Y, Z) tambien se añade:
- La información del coseno del ángulo incidente. 
- Un identificador del objeto.
- Una etiqueta semántica.

Por lo tanto, se ha adaptado el código para procesar esta información, permitiendo su visualización en tiempo real y su almacenamiento para la creación de datasets.

```python
# Obtención del color según la etiqueta semántica
def get_color_from_semantic(semantic_tag):
    return SEMANTIC_COLOR_MAP.get(semantic_tag, (255, 255, 255))  # Color blanco si no está en la lista

# Callback para procesar los datos del sensor LiDAR
def lidar_callback(lidar_data, point_cloud, frame):
    data = np.copy(np.frombuffer(lidar_data.raw_data, dtype=np.dtype('f4')))
    data = np.reshape(data, (int(data.shape[0] / 6), 6))  # Ahora cada fila tiene 6 valores

    # Reflejar los datos en el eje X
    data[:, 0] = -data[:, 0]

    # Extraer etiquetas semánticas
    semantic_tags = data[:, 5].view(np.uint32)  # Ver datos como enteros

    # Asignar colores según etiquetas
    colors = np.array([get_color_from_semantic(tag) for tag in semantic_tags]) / 255.0  # Normalizar a [0,1]

    point_cloud.points = o3d.utility.Vector3dVector(data[:, :3])  # Nube de puntos (Nx3).
    point_cloud.colors = o3d.utility.Vector3dVector(colors) # Colores RGB normalizados (Nx3)

    # Guardar las etiquetas semánticas en el campo "normals"
    point_cloud.normals = o3d.utility.Vector3dVector(np.c_[semantic_tags, semantic_tags, semantic_tags])

    output_dir = 'dataset/lidar'
    # Crear la carpeta de salida si no existe
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    # Guardar el point cloud cada ciertos frames (por ejemplo, cada 20 frames)
    if frame % 20 == 0:
        filename = os.path.join(output_dir, f"lidar_points_{frame:04d}.ply")
        print(f"Guardando archivo {filename}...")
        o3d.io.write_point_cloud(filename, point_cloud)
```

### Resultado

<p align="center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/60U6XWd-LK8?si=HH8b3-VN4rF6NNd2" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
</p>

## Comparación con datasets
Se ha creado un pequeño escenario en CARLA para replicar la secuencia 00000 del dataset Rellis-3D con el fin de evaluar la fiabilidad del LiDAR semántico para la generación de datasets sintéticos.

### Rellis-3D Dataset: sequence 00000
<p align="center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/Qc7IepWGKr8?si=TNyCThzWyt7INwEY" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
</p>

### Recreación en CARLA
<p align="center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/HXlQnljTnjQ?si=O1qJ4eLO0EA5hSd4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
</p>

### Conclusión
El sensor LiDAR de CARLA ofrece resultados bastante óptimos para la creación de datasets sintéticos, logrando una mayor precisión en ciertos aspectos en comparación con datasets reales, donde influyen múltiples factores externos.
Sin embargo, presenta algunas limitaciones, como la falsa hitbox de ciertos objetos. En el video se observa que los árboles, aunque en la simulación apenas tienen hojas y solo muestran ramas, aparecen como árboles frondosos en los datos capturados por el sensor. Esto probablemente se deba a una simplificación en la simulación para reducir el coste computacional.
Aun así, este problema no supone un gran inconveniente, ya que los sensores LiDAR en datasets reales tampoco son completamente precisos y presentan errores similares en la captura de objetos. Además, en la mayoría de aplicaciones, no es necesaria una medición exacta del contorno de los objetos, sino una representación suficiente para la segmentación y el entrenamiento de modelos.

